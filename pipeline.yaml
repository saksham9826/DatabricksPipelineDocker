apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: pyspark-pipeline40
  namespace: default
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "pipelineimage6:latest"
  imagePullPolicy: IfNotPresent
  deps:
    pyFiles:
      - local:///venv1.zip
    jars:
      - local:///scalaSpark-assembly-1.0.jar
  mainApplicationFile: local:///pipeline.py
  arguments:
    - "ab.txt"
  sparkVersion: "3.0.0"
  sparkConf:
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "file:/data2"
    spark.jars.packages: "org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql_2.12:3.0.0,org.apache.kafka:kafka-clients:2.8.0,org.apache.spark:spark-avro_2.12:3.0.0,io.delta:delta-core_2.12:0.8.0,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-streaming_2.12:3.0.0"
    spark.pyspark.python: "python3"
  restartPolicy:
    type: Never
  volumes:
    - name: "test-volume2"
      persistentVolumeClaim:
        claimName: spark-pvc3
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "522m"
    labels:
      version: 3.0.0
    serviceAccount: gcpspark
    volumeMounts:
      - name: "test-volume2"
        mountPath: "/data2"
    securityContext:
      allowPrivilegeEscalation: false
      runAsUser: 0
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    labels:
      version: 3.0.0
    securityContext:
      allowPrivilegeEscalation: false
      runAsUser: 0
    volumeMounts:
      - name: "test-volume2"
        mountPath: "/data2"
